{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import col, concat_ws, lit, substring_index\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadParquetExample\").getOrCreate()\n",
    "\n",
    "from delta import *\n",
    "\n",
    "# Configurações de conexão com o banco de dados\n",
    "db_params = {\n",
    "    'host': 'psql-mock-database-cloud.postgres.database.azure.com',\n",
    "    'port': '5432',\n",
    "    'dbname': 'ecom1692637467959vmhtnfhkniqcdrvy',\n",
    "    'user': 'sblzxqbkzxogfxlutfgnyycw@psql-mock-database-cloud',\n",
    "    'password': 'qvcedokxqkibwxtcclvgfmte'\n",
    "}\n",
    "\n",
    "# JDBC URL for PostgreSQL\n",
    "jdbc_url = f\"jdbc:postgresql://{db_params['host']}:{db_params['port']}/{db_params['dbname']}\"\n",
    "\n",
    "# JDBC connection properties\n",
    "properties = {\n",
    "    'user': db_params['user'],\n",
    "    'password': db_params['password'],\n",
    "    'driver': 'org.postgresql.Driver'\n",
    "}\n",
    "\n",
    "pasta_base = 'delta'\n",
    "\n",
    "try:\n",
    "    os.mkdir(pasta_base)\n",
    "except:\n",
    "    shutil.rmtree(pasta_base)\n",
    "    os.mkdir(pasta_base)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customers', 'employees', 'offices', 'orderdetails', 'orders', 'payments', 'product_lines', 'products', 'pg_stat_statements', 'pg_buffercache']\n"
     ]
    }
   ],
   "source": [
    "# Conecta ao banco de dados\n",
    "conn = psycopg2.connect(**db_params)\n",
    "\n",
    "# Cria um cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Executa a consulta para listar as tabelas\n",
    "query = \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\"\n",
    "cursor.execute(query)\n",
    "\n",
    "# Obtém os resultados\n",
    "tabelas = [row[0] for row in cursor.fetchall()]\n",
    "print(tabelas)\n",
    "# Fecha o cursor\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850977/1049969471.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query_tabela, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** Tabela customers salva como Parquet em delta/customers/customers.parquet ***\n",
      "\n",
      " *** Tabela employees salva como Parquet em delta/employees/employees.parquet ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850977/1049969471.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query_tabela, conn)\n",
      "/tmp/ipykernel_850977/1049969471.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query_tabela, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** Tabela offices salva como Parquet em delta/offices/offices.parquet ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850977/1049969471.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query_tabela, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** Tabela orderdetails salva como Parquet em delta/orderdetails/orderdetails.parquet ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850977/1049969471.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query_tabela, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** Tabela orders salva como Parquet em delta/orders/orders.parquet ***\n",
      "\n",
      " *** Tabela payments salva como Parquet em delta/payments/payments.parquet ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850977/1049969471.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query_tabela, conn)\n",
      "/tmp/ipykernel_850977/1049969471.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query_tabela, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** Tabela product_lines salva como Parquet em delta/product_lines/product_lines.parquet ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850977/1049969471.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query_tabela, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** Tabela products salva como Parquet em delta/products/products.parquet ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850977/1049969471.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query_tabela, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** Tabela pg_stat_statements salva como Parquet em delta/pg_stat_statements/pg_stat_statements.parquet ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_850977/1049969471.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query_tabela, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** Tabela pg_buffercache erro ao salvar como Parquet em delta/pg_stat_statements/pg_stat_statements.parquet ***\n"
     ]
    }
   ],
   "source": [
    "# Cria uma pasta para os arquivos Parquet (se não existir)\n",
    "pasta_base = 'delta/'\n",
    "tabelas_suc = []\n",
    "# Itera sobre as tabelas e salva cada uma em uma pasta separada\n",
    "for tabela in tabelas:\n",
    "    try:\n",
    "        query_tabela = f\"SELECT * FROM {tabela}\"\n",
    "        df = pd.read_sql(query_tabela, conn)\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # Cria uma pasta para a tabela (se não existir)\n",
    "        pasta_tabela = os.path.join(pasta_base, tabela)\n",
    "        if not os.path.exists(pasta_tabela):\n",
    "            os.makedirs(pasta_tabela)\n",
    "\n",
    "        caminho_parquet = os.path.join(pasta_tabela, f'{tabela}.parquet')\n",
    "        pq.write_table(table, caminho_parquet)\n",
    "        print(f'\\n *** Tabela {tabela} salva como Parquet em {caminho_parquet} ***')\n",
    "        tabelas_suc.append(tabela)\n",
    "    except:\n",
    "        print(f'\\n *** Tabela {tabela} erro ao salvar como Parquet em {caminho_parquet} ***')\n",
    "\n",
    "    #print(f'\\n *** Tabela {tabela} não salva como Delta ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customers', 'employees', 'offices', 'orderdetails', 'orders', 'payments', 'product_lines', 'products', 'pg_stat_statements']\n",
      "Código DBML gerado e salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "parquet_files = []\n",
    "\n",
    "print(tabelas_suc)\n",
    "for tabela in tabelas_suc:\n",
    "    pasta_tabela = os.path.join(pasta_base, tabela)\n",
    "    caminho_parquet = os.path.join(pasta_tabela, f'{tabela}.parquet')\n",
    "    parquet_files.append(caminho_parquet)\n",
    "\n",
    "dataframes = [pd.read_parquet(file) for file in parquet_files]\n",
    "\n",
    "# Mapear tipos de dados Pandas para tipos DBML\n",
    "pandas_to_dbml_types = {\n",
    "    'int64': 'integer',\n",
    "    'float64': 'float',\n",
    "    'object': 'varchar',\n",
    "    'datetime64': 'timestamp',\n",
    "    'bool': 'boolean',\n",
    "}\n",
    "\n",
    "# Crie o código DBML a partir dos DataFrames\n",
    "pandas_to_dbml_types = {\n",
    "    'int64': 'integer',\n",
    "    'float64': 'float',\n",
    "    'object': 'varchar',\n",
    "    'datetime64': 'timestamp',\n",
    "    'bool': 'boolean',\n",
    "}\n",
    "\n",
    "# Crie o código DBML a partir dos DataFrames\n",
    "dbml_code = \"\"\n",
    "tables = []\n",
    "\n",
    "for idx, df in enumerate(dataframes):\n",
    "    table_name = tabelas_suc[idx]\n",
    "    table_code = f\"Table {table_name} {{\\n\"\n",
    "    columns = []\n",
    "\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        pandas_type = str(dtype)\n",
    "        dbml_type = pandas_to_dbml_types.get(pandas_type, 'varchar')\n",
    "        columns.append((col, dbml_type))\n",
    "\n",
    "    table_code += f\"  {columns[0][0]} {columns[0][1]} [primary key]\\n\"\n",
    "    for col_name, col_type in columns[1:]:\n",
    "        table_code += f\"  {col_name} {col_type}\\n\"\n",
    "\n",
    "    table_code += \"}\\n\\n\"\n",
    "    dbml_code += table_code\n",
    "    tables.append((table_name, columns))\n",
    "\n",
    "# Gerar relações automaticamente com base nas semelhanças de nomes\n",
    "for i, (table_name, columns) in enumerate(tables):\n",
    "    for j, (other_table_name, other_columns) in enumerate(tables):\n",
    "        if i != j:\n",
    "            for col_name, _ in columns:\n",
    "                for other_col_name, _ in other_columns:\n",
    "                    if col_name == other_col_name:\n",
    "                        dbml_code += f\"Ref: {table_name}.{col_name} < {other_table_name}.{other_col_name}\\n\"\n",
    "                        dbml_code += f\"Ref: {other_table_name}.{other_col_name} < {table_name}.{col_name}\\n\"\n",
    "\n",
    "# Salvar o código DBML em um arquivo\n",
    "with open('database.dbml', 'w') as dbml_file:\n",
    "    dbml_file.write(dbml_code)\n",
    "\n",
    "print(\"Código DBML gerado e salvo com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual país possui a maior quantidade de itens cancelados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta customer\n",
    "parquet_file_path = 'delta/customers/customers.parquet'\n",
    "df_customer = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Pasta orders\n",
    "parquet_file_path = 'delta/orders/orders.parquet'\n",
    "df_orders = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Merge\n",
    "df_customer_orders = df_orders.join(df_customer, on='customer_number', how='left')\n",
    "\n",
    "df_customer_orders_cancel = df_customer_orders[df_customer_orders['status']=='Cancelled']\n",
    "country_value_counts = df_customer_orders_cancel.groupBy('country').count()\n",
    "country_value_counts_sorted = country_value_counts.orderBy(desc('count'))\n",
    "country_value_counts_sorted.show()\n",
    "first_country_name = country_value_counts_sorted.select('country').first()[0]\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Venda mais realizada:\", first_country_name)\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual o faturamento da linha de produto mais vendido, considere como os itens Shipped, cujo o pedido foi realizado no ano de 2005?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta orderdetails\n",
    "parquet_file_path = 'delta/orderdetails/orderdetails.parquet'\n",
    "df_orderdetails = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Pasta orders\n",
    "parquet_file_path = 'delta/orders/orders.parquet'\n",
    "df_orders = spark.read.parquet(parquet_file_path)\n",
    "df_orders = df_orders[df_orders['status']=='Shipped']\n",
    "df_orders = df_orders[df_orders['order_date'] >= '2005-01-01']\n",
    "df_orders = df_orders[df_orders['order_date'] < '2006-01-01']\n",
    "\n",
    "# Pasta products\n",
    "parquet_file_path = 'delta/products/products.parquet'\n",
    "df_products = spark.read.parquet(parquet_file_path)\n",
    "print(df_products.columns)\n",
    "\n",
    "# Merge\n",
    "df_order_orderdetails = df_orders.join(df_orderdetails, on='order_number', how='left')\n",
    "df_order_orderdetails = df_order_orderdetails.join(df_products[['product_name','product_code','buy_price']], on='product_code', how='left')\n",
    "print(df_order_orderdetails.columns)\n",
    "\n",
    "product_value_counts = df_order_orderdetails.groupBy('product_name').count()\n",
    "product_value_counts_sorted = product_value_counts.orderBy(desc('count'))\n",
    "\n",
    "first_product_name = product_value_counts_sorted.select('product_name').first()[0]\n",
    "df_order_orderdetails = df_order_orderdetails[df_order_orderdetails['product_name']==first_product_name]\n",
    "\n",
    "df_with_product = df_order_orderdetails.withColumn(\"product_total\", col(\"quantity_ordered\") * col(\"price_each\"))\n",
    "total_buy_price = df_with_product.agg({\"product_total\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Venda mais realizada:\", first_product_name)\n",
    "print(\"Total:\", total_buy_price)\n",
    "print(\"--------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome, sobrenome e e-mail dos vendedores do Japão, o local-part do e-mail deve estar mascarado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta employees\n",
    "parquet_file_path = 'delta/employees/employees.parquet'\n",
    "df_employees = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Pasta employees\n",
    "parquet_file_path = 'delta/offices/offices.parquet'\n",
    "df_offices = spark.read.parquet(parquet_file_path)\n",
    "df_employees_offices = df_employees.join(df_offices, on='office_code', how='left')\n",
    "df_vendedores_japao = df_employees_offices.filter(col(\"country\") == \"Japan\")\n",
    "masked_email = concat_ws('@', substring_index(col(\"email\"), '@', 1), lit(\"*****\"))\n",
    "df_resultado = df_vendedores_japao.select(\"first_name\", \"last_name\", masked_email.alias(\"masked_email\"))\n",
    "df_resultado.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
